{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad00849e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Yelp Review Dataset\n",
    "\n",
    "In this tutorial notebook, I am going to learn from Delip Rao and Brian McMahan's book \"Natural Language Processing with PyTorch\" and modify it to further understand it. We will be building a Sentiment Analyzer for the Yelp Review Dataset. \n",
    "\n",
    "First, we have to split the dataset into three sets: Training, Validation and Testing. \n",
    "\n",
    "From training dataset, our model will derive parameters, with validation set, our model can make decisions (by selecting among hyperparameters) and the testing set for final evaluation.\n",
    "\n",
    "I have downloaded the Dataset from this [link](https://www.kaggle.com/datasets/ilhamfp31/yelp-review-dataset)\n",
    "\n",
    "I am storing this under the `/data` folder under the name `yelp_review`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5fb376",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e394f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f860f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f7c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851f9ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_dir = \"data/yelp_review/\"\n",
    "train_dataset = pd.read_csv(data_base_dir+\"train.csv\", header = None)\n",
    "test_dataset = pd.read_csv(data_base_dir+\"test.csv\", header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c2c78d",
   "metadata": {},
   "source": [
    "We have to see the distribution of data, having uneven data will make our model more biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1132e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.columns = [\"Rating\", \"Review\"]\n",
    "test_dataset.columns = [\"Rating\", \"Review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "597b4157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 560000 entries, 0 to 559999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   Rating  560000 non-null  int64 \n",
      " 1   Review  560000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 8.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076e101d",
   "metadata": {},
   "source": [
    "There are 560,000 review in our dataset. Let us see the distribution for Positive reviews and Negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ffc2bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    280000\n",
       "2    280000\n",
       "Name: Rating, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"Rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8ba45e",
   "metadata": {},
   "source": [
    "It looks like we have equal distribution, we need to take a subset of this Dataset, about `10%` with the same distribution. Before we do that, let us check the distribution of `test_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff4b6998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    19000\n",
       "1    19000\n",
       "Name: Rating, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[\"Rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9223ceeb",
   "metadata": {},
   "source": [
    "Even `test_dataset` has the same equal distribution, we want to be able to create three sets: Train, Val, Test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19866655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                                             Review\n",
       "0       1  Unfortunately, the frustration of being Dr. Go...\n",
       "1       2  Been going to Dr. Goldberg for over 10 years. ...\n",
       "2       1  I don't know what Dr. Goldberg was like before...\n",
       "3       1  I'm writing this review to give you a heads up...\n",
       "4       2  All the food is great here. But the best thing..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dataset = pd.concat([train_dataset, test_dataset], ignore_index= True)\n",
    "main_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dff12fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    299000\n",
       "2    299000\n",
       "Name: Rating, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dataset[\"Rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c31a1",
   "metadata": {},
   "source": [
    "I have combined both the Training dataset and the Testing dataset. We are going to create three new subsets from these. \n",
    "\n",
    "`Train - 70%, Val - 15%, Test - 15%`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3e0debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset = main_dataset.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e962619d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>I'm so glad my friends told me to go here!! Wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I was really looking forward to trying this pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I didnt know \\\"sh*tty wok\\\" from south park ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>I was looking for a pizza delivery place w/ mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Solid breakfast food.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                                             Review\n",
       "0       2  I'm so glad my friends told me to go here!! Wo...\n",
       "1       1  I was really looking forward to trying this pl...\n",
       "2       1  I didnt know \\\"sh*tty wok\\\" from south park ex...\n",
       "3       2  I was looking for a pizza delivery place w/ mo...\n",
       "4       2                              Solid breakfast food."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4c1b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset = main_dataset[:int(0.1*len(main_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dd689db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59800\n"
     ]
    }
   ],
   "source": [
    "split = []\n",
    "dataset_size = len(main_dataset)\n",
    "train_rec = int(0.7 * dataset_size)\n",
    "test_rec = int(0.15 * dataset_size)\n",
    "val_rec = dataset_size - train_rec - test_rec\n",
    "for i in range(train_rec):\n",
    "    split.append(\"train\")\n",
    "for i in range(test_rec):\n",
    "    split.append(\"test\")\n",
    "for i in range(val_rec):\n",
    "    split.append(\"val\")\n",
    "    \n",
    "print(len(split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12ed9c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset[\"split\"] = split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8edd9a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    41860\n",
       "test      8970\n",
       "val       8970\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dataset[\"split\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c74ae07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    29990\n",
       "2    29810\n",
       "Name: Rating, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dataset[\"Rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb1410c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_dataset(review):\n",
    "    review = review.lower()\n",
    "    review = re.sub(r'([.,?!])', r' \\1', review)\n",
    "    review = re.sub(r'([^a-zA-Z.,!?])', r' ', review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8bef104",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset[\"Review\"] = main_dataset[\"Review\"].apply(cleaning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8f585b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>i m so glad my friends told me to go here ! ! ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i was really looking forward to trying this pl...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i didnt know   sh tty wok   from south park ex...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>i was looking for a pizza delivery place w  mo...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>solid breakfast food .</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                                             Review  split\n",
       "0       2  i m so glad my friends told me to go here ! ! ...  train\n",
       "1       1  i was really looking forward to trying this pl...  train\n",
       "2       1  i didnt know   sh tty wok   from south park ex...  train\n",
       "3       2  i was looking for a pizza delivery place w  mo...  train\n",
       "4       2                             solid breakfast food .  train"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85a3cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset.to_csv(\"data/yelp_review/reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c211cfb",
   "metadata": {},
   "source": [
    "## 2. Vocabulary\n",
    "Think of this as two dictionaries, one mapping tokens to ids and another mappings ids to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42f6efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    # Extracts vocab for mapping\n",
    "    def __init__(self, token_to_idx = None, add_unk = True, unk_token = \"<UNK>\"):\n",
    "        # token_to_idx is pre-existent mapping of tokens to index\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        self._idx_to_token = {idx : token for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "            \n",
    "    def to_serializable(self):\n",
    "        # returns a serializable dictionary (ordered)\n",
    "        return {\n",
    "            \"token_to_idx\" : self._token_to_idx,\n",
    "            \"add_unk\" : self._add_unk,\n",
    "            \"unk_token\" : self._unk_token\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        # Instantiate a vocab from serialized dictionary\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"Index not present\")\n",
    "        else:\n",
    "            return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"<Vocabulary(size={len(self)})>\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e9827",
   "metadata": {},
   "source": [
    "## 3. Vectorizer\n",
    "Converts reviews (texts) to vectors. It does Collapsed One Hot representation. We don't really account for the semantic meaning or the number of occurences (we do have a CUT_OFF). We only care if the word is present in the review, not how many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa7efe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    def __init__(self, review_vocab, rating_vocab):\n",
    "        self.review_vocab = review_vocab # Maps words to integers\n",
    "        self.rating_vocab = rating_vocab # Maps class labels to integers\n",
    "        \n",
    "    def vectorize(self, review):\n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype = np.float32)\n",
    "        \n",
    "        for token in review.split(\" \"):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "                \n",
    "        return one_hot\n",
    "    \n",
    "    @classmethod\n",
    "    def from_df(cls, df, cutoff = 25):\n",
    "        review_vocab = Vocabulary(add_unk= True)\n",
    "        rating_vocab = Vocabulary(add_unk= False)\n",
    "        \n",
    "        # Add ratings to rating_vocab\n",
    "        for rating in sorted(set(df.Rating)):\n",
    "            rating_vocab.add_token(rating)\n",
    "            \n",
    "        # Cross threshold add word\n",
    "        word_counts = Counter()\n",
    "        for review in df.Review:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "            \n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "                \n",
    "        return cls(review_vocab, rating_vocab)\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        review_vocab = Vocabulary.from_serializable(contents[\"review_vocab\"])\n",
    "        rating_vocab = Vocabulary.from_serializable(contents[\"rating_vocab\"])\n",
    "        \n",
    "        return cls(review_vocab = review_vocab, rating_vocab = rating_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        # Serializable dictionary for caching (IDK what this means, to be honest)\n",
    "        return {\n",
    "            \"review_vocab\" : self.review_vocab.to_serializable(),\n",
    "            \"rating_vocab\" : self.rating_vocab.to_serializable()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287097b1",
   "metadata": {},
   "source": [
    "## 4. Dataset\n",
    "\n",
    "We denote the entry point method (from where the data flows in) with the `@classmethod` decorator. From what I comprehend, Decorators are essentially function passed into another function. It extends the behaviour of the function that is getting passed in without explicitly modifying the code. Decorators wrap a function and modify its behaviour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b7c2e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpReviewDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer):\n",
    "        self.df = df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        # We are going to split our dataset into the three sets: Train, Val and Test\n",
    "        # Train Split\n",
    "        self.train_df = self.df[self.df.split == \"train\"]\n",
    "        self.train_size = len(self.train_df)\n",
    "        \n",
    "        # Val Split\n",
    "        self.val_df = self.df[self.df.split == \"val\"]\n",
    "        self.val_size = len(self.val_df)\n",
    "        \n",
    "        # Test Split\n",
    "        self.test_df = self.df[self.df.split == \"test\"]\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self._lookup_dict = {\"train\" : (self.train_df, self.train_size),\n",
    "                             \"val\" : (self.val_df, self.val_size),\n",
    "                             \"test\" : (self.test_df, self.test_size)}\n",
    "        \n",
    "        # By default, we will be in Train set\n",
    "        self.set_split(\"train\")\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls,csv_location):\n",
    "        df = pd.read_csv(csv_location)\n",
    "        train_review_df = df[df.split=='train']\n",
    "        return cls(df, ReviewVectorizer.from_df(train_review_df))\n",
    "\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self, split = \"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # This function specifies the size of the dataset\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Takes in index and returns features and labels\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        review_vector = self._vectorizer.vectorize(row[\"Review\"])\n",
    "        \n",
    "        rating_index = self._vectorizer.rating_vocab.lookup_token(row[\"Rating\"])\n",
    "        \n",
    "        return {\"x_data\" : review_vector, \"y_target\" : rating_index}\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4aa09",
   "metadata": {},
   "source": [
    "## 5. DataLoader\n",
    "We are grouping the data points for Batch Training. Our dataloader in called `generate_batches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86215c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle = True, drop_last = True, device = \"cpu\"):\n",
    "    dataloader = DataLoader(dataset = dataset, batch_size = batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    \n",
    "    for data_dict in dataloader:\n",
    "        # Port it to device\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d2a3fe",
   "metadata": {},
   "source": [
    "## 6. A simple Perceptron\n",
    "One hidden layer with sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e95381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6dd27e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        # num_features is size of the input feature vector\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, out_features= 1)\n",
    "    def forward(self, x_in, apply_sigmoid = False):\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = F.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da67dcf",
   "metadata": {},
   "source": [
    "## 7. Training Setup and Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c972db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f0ecc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path\n",
    "    frequency_cuttoff = 25, \n",
    "    model_state_file = \"model.pth\",\n",
    "    csv_location = \"data/yelp_review/reviews.csv\",\n",
    "    save_dir = \"/\",\n",
    "    # Training hyperparams\n",
    "    batch_size = 128,\n",
    "    early_stopping_criteria = 5,\n",
    "    learning_rate = 0.001,\n",
    "    num_epochs = 100,\n",
    "    seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d476e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0970d91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {\n",
    "        \"epoch_index\" : 0,\n",
    "        \"train_loss\" : [],\n",
    "        \"train_acc\" : [],\n",
    "        \"val_loss\" : [],\n",
    "        \"val_acc\" : [],\n",
    "        \"test_loss\" : -1,\n",
    "        \"test_acc\" : -1,\n",
    "    }\n",
    "\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "30fe1e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e71e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the Dataset and Vectorizer\n",
    "dataset = YelpReviewDataset.load_dataset_and_make_vectorizer(args.csv_location)\n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "58cf6245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "classifier = ReviewClassifier(num_features= len(vectorizer.review_vocab))\n",
    "classifier = classifier.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e8a9d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and Optimizer\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "87a820c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b555b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state[\"epoch_index\"] = epoch_index\n",
    "    \n",
    "    # Iterating over training dataset\n",
    "    dataset.set_split(\"train\")\n",
    "    \n",
    "    batch_generator = generate_batches(dataset, batch_size= args.batch_size, device= args.device)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # Training routine is 5 steps\n",
    "        # Step 1: Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Step 2: Compute output\n",
    "        y_pred = classifier(x_in = batch_dict[\"x_data\"].float())\n",
    "        \n",
    "        # Step 3: Compute Loss\n",
    "        loss = loss_func(y_pred, batch_dict[\"y_target\"].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "        \n",
    "        # Step 4: Use loss to produce gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 5: Use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Computing accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict[\"y_target\"])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "        \n",
    "    train_state[\"train_loss\"].append(running_loss)\n",
    "    train_state[\"train_acc\"].append(running_acc)\n",
    "    \n",
    "    # Iterating over val dataset\n",
    "    dataset.set_split(\"val\")\n",
    "    \n",
    "    batch_generator = generate_batches(dataset, batch_size= args.batch_size, device= args.device)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # Training routine is 5 steps\n",
    "        # Step 1: Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Step 2: Compute output\n",
    "        y_pred = classifier(x_in = batch_dict[\"x_data\"].float())\n",
    "        \n",
    "        # Step 3: Compute Loss\n",
    "        loss = loss_func(y_pred, batch_dict[\"y_target\"].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "        \n",
    "        # Step 4: Use loss to produce gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 5: Use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Computing accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict[\"y_target\"])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "        \n",
    "    train_state[\"val_loss\"].append(running_loss)\n",
    "    train_state[\"val_acc\"].append(running_acc)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "649ab83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch_index': 99,\n",
       " 'train_loss': [0.4710985072162175,\n",
       "  0.3074474175679938,\n",
       "  0.2562686162986522,\n",
       "  0.22865879604029013,\n",
       "  0.21080981354465544,\n",
       "  0.19784240704884212,\n",
       "  0.18809375834483258,\n",
       "  0.18008661867099438,\n",
       "  0.17350872462495753,\n",
       "  0.16779210413905823,\n",
       "  0.16288448984105303,\n",
       "  0.15863256498214312,\n",
       "  0.1548691922371541,\n",
       "  0.15126183722453018,\n",
       "  0.14826552239488763,\n",
       "  0.14525991347860492,\n",
       "  0.14262602970414215,\n",
       "  0.14018622068090172,\n",
       "  0.13796529454102222,\n",
       "  0.1359060880149907,\n",
       "  0.13382846572712664,\n",
       "  0.13194971761754531,\n",
       "  0.13029966166168913,\n",
       "  0.12855416096587433,\n",
       "  0.12700061440832386,\n",
       "  0.125540496103625,\n",
       "  0.12408057980763444,\n",
       "  0.12275315807500024,\n",
       "  0.1214307383819275,\n",
       "  0.12019194511053033,\n",
       "  0.11905493744469564,\n",
       "  0.11784644658457977,\n",
       "  0.11667327708151724,\n",
       "  0.11569835105483686,\n",
       "  0.11463970232994192,\n",
       "  0.11356221071077052,\n",
       "  0.11267241256864056,\n",
       "  0.11175121456977062,\n",
       "  0.1110219381840769,\n",
       "  0.10996307916825339,\n",
       "  0.10920918667945291,\n",
       "  0.10835462413295323,\n",
       "  0.10762507114160681,\n",
       "  0.10682473946875391,\n",
       "  0.10607832388821369,\n",
       "  0.1054408953778605,\n",
       "  0.10467992873366826,\n",
       "  0.10409612251895649,\n",
       "  0.1034111691473639,\n",
       "  0.10270723241412677,\n",
       "  0.10207403758797082,\n",
       "  0.10143831894029531,\n",
       "  0.10084510988780851,\n",
       "  0.1003069779878362,\n",
       "  0.09970990977383899,\n",
       "  0.09917505274580893,\n",
       "  0.09865397197801039,\n",
       "  0.09802933502087902,\n",
       "  0.09764668015073927,\n",
       "  0.0969812044082067,\n",
       "  0.09657903164853011,\n",
       "  0.09601876620396929,\n",
       "  0.09557458995314547,\n",
       "  0.0951353833705916,\n",
       "  0.09466176101829664,\n",
       "  0.09423204947730088,\n",
       "  0.09381641955641798,\n",
       "  0.09332126504328632,\n",
       "  0.09291511497229617,\n",
       "  0.09248859185827986,\n",
       "  0.09205963945625754,\n",
       "  0.09174082417963841,\n",
       "  0.09128819884510213,\n",
       "  0.0908433785405727,\n",
       "  0.09053928733145421,\n",
       "  0.09016899480868912,\n",
       "  0.08975777100623558,\n",
       "  0.08931276888019803,\n",
       "  0.08909972982669094,\n",
       "  0.08868349025692418,\n",
       "  0.0883052459596129,\n",
       "  0.08802019475477911,\n",
       "  0.0876482595498773,\n",
       "  0.08730094978978875,\n",
       "  0.08698840090714459,\n",
       "  0.08668935409425235,\n",
       "  0.08634532579086977,\n",
       "  0.08607489694285826,\n",
       "  0.08573450044435454,\n",
       "  0.08546210948480379,\n",
       "  0.08521725690784067,\n",
       "  0.08485352546432326,\n",
       "  0.08458047298663263,\n",
       "  0.08428478868683907,\n",
       "  0.08401005715131761,\n",
       "  0.08370265877301543,\n",
       "  0.08341708098073983,\n",
       "  0.08314161293140246,\n",
       "  0.08293195987057614,\n",
       "  0.082743690329225],\n",
       " 'train_acc': [84.7596521406727,\n",
       "  90.88302752293578,\n",
       "  92.17077599388378,\n",
       "  92.85407110091741,\n",
       "  93.16465978593276,\n",
       "  93.53736620795108,\n",
       "  93.78583715596328,\n",
       "  93.98413608562686,\n",
       "  94.2039373088686,\n",
       "  94.39506880733941,\n",
       "  94.55991972477068,\n",
       "  94.71760321100913,\n",
       "  94.81077981651372,\n",
       "  94.90634556574922,\n",
       "  95.01624617736996,\n",
       "  95.14525993883792,\n",
       "  95.2145451070336,\n",
       "  95.31011085626913,\n",
       "  95.37700688073396,\n",
       "  95.42240061162073,\n",
       "  95.50124235474003,\n",
       "  95.60875382262999,\n",
       "  95.62069954128437,\n",
       "  95.73776758409782,\n",
       "  95.71865443425075,\n",
       "  95.74254587155967,\n",
       "  95.84050076452598,\n",
       "  95.85961391437316,\n",
       "  95.9695145259938,\n",
       "  95.98623853211011,\n",
       "  96.01251911314985,\n",
       "  96.05791284403662,\n",
       "  96.13914373088684,\n",
       "  96.15825688073389,\n",
       "  96.18214831804285,\n",
       "  96.24187691131495,\n",
       "  96.29921636085626,\n",
       "  96.28727064220185,\n",
       "  96.31355122324166,\n",
       "  96.35416666666659,\n",
       "  96.41389525993876,\n",
       "  96.43300840978586,\n",
       "  96.48079128440367,\n",
       "  96.51901758409777,\n",
       "  96.55007645259938,\n",
       "  96.56441131498468,\n",
       "  96.57157874617745,\n",
       "  96.64086391437301,\n",
       "  96.63130733944956,\n",
       "  96.66953363914375,\n",
       "  96.66475535168195,\n",
       "  96.69581422018358,\n",
       "  96.71970565749237,\n",
       "  96.71731651376156,\n",
       "  96.75076452599399,\n",
       "  96.79854740061157,\n",
       "  96.83438455657495,\n",
       "  96.82482798165134,\n",
       "  96.81288226299694,\n",
       "  96.85110856269108,\n",
       "  96.84394113149848,\n",
       "  96.90128058103977,\n",
       "  96.91561544342507,\n",
       "  96.92756116207943,\n",
       "  96.93233944954127,\n",
       "  96.9849006116208,\n",
       "  96.96339831804276,\n",
       "  96.99206804281346,\n",
       "  96.99684633027516,\n",
       "  97.05179663608565,\n",
       "  97.03507262996942,\n",
       "  97.04462920489307,\n",
       "  97.07807721712534,\n",
       "  97.09002293577979,\n",
       "  97.07807721712537,\n",
       "  97.09480122324156,\n",
       "  97.13302752293576,\n",
       "  97.15452981651376,\n",
       "  97.1879778287462,\n",
       "  97.15452981651383,\n",
       "  97.18081039755339,\n",
       "  97.16408639143732,\n",
       "  97.20470183486239,\n",
       "  97.22381498470945,\n",
       "  97.21186926605502,\n",
       "  97.22142584097851,\n",
       "  97.24292813455655,\n",
       "  97.25248470948009,\n",
       "  97.28593272171254,\n",
       "  97.24292813455658,\n",
       "  97.29787844036697,\n",
       "  97.3146024464832,\n",
       "  97.34566131498474,\n",
       "  97.32415902140673,\n",
       "  97.36716360856273,\n",
       "  97.37910932721712,\n",
       "  97.35282874617734,\n",
       "  97.37194189602432,\n",
       "  97.39344418960239,\n",
       "  97.38388761467886],\n",
       " 'val_loss': [0.3576418834073203,\n",
       "  0.279059330693313,\n",
       "  0.24240277281829287,\n",
       "  0.2200068150247846,\n",
       "  0.20455939578158525,\n",
       "  0.19289084140743532,\n",
       "  0.18375549039670397,\n",
       "  0.17644523243818963,\n",
       "  0.16994423259581837,\n",
       "  0.16441778742841318,\n",
       "  0.1595890713589532,\n",
       "  0.15570450818964415,\n",
       "  0.15200526128922182,\n",
       "  0.14850016087293624,\n",
       "  0.14578079123582158,\n",
       "  0.14307614341378216,\n",
       "  0.14007283651403019,\n",
       "  0.13798197997467862,\n",
       "  0.13556443229317663,\n",
       "  0.13380460504974634,\n",
       "  0.13187269174626903,\n",
       "  0.1299846617238862,\n",
       "  0.12837330720254356,\n",
       "  0.12675622818725452,\n",
       "  0.1254233163382326,\n",
       "  0.12392997539469174,\n",
       "  0.12237677137766564,\n",
       "  0.12121571760092464,\n",
       "  0.11994588683758468,\n",
       "  0.11873756104281973,\n",
       "  0.11727059089711732,\n",
       "  0.11650084384850096,\n",
       "  0.11573587911469597,\n",
       "  0.11438533418944904,\n",
       "  0.1136712521314621,\n",
       "  0.11302075018840176,\n",
       "  0.11196376532316209,\n",
       "  0.11063228100538251,\n",
       "  0.10964389555156234,\n",
       "  0.10940537921019959,\n",
       "  0.10840384608932903,\n",
       "  0.10784519352018834,\n",
       "  0.10673971718975477,\n",
       "  0.10620990033660618,\n",
       "  0.10523317738303115,\n",
       "  0.10453733493174824,\n",
       "  0.1039437811289515,\n",
       "  0.10328131711908749,\n",
       "  0.10274022040622573,\n",
       "  0.10220720821193287,\n",
       "  0.10153641322893757,\n",
       "  0.10084449494523665,\n",
       "  0.10059235590909209,\n",
       "  0.09982006576444423,\n",
       "  0.09917457838143622,\n",
       "  0.0988235665751355,\n",
       "  0.09822809712163036,\n",
       "  0.0974973960646561,\n",
       "  0.09708782865532808,\n",
       "  0.09679614475795204,\n",
       "  0.09625197611749176,\n",
       "  0.09563830393765656,\n",
       "  0.0952716987580061,\n",
       "  0.09460413439997606,\n",
       "  0.09446107835641931,\n",
       "  0.09356535172888208,\n",
       "  0.09345402754843239,\n",
       "  0.09300177044102124,\n",
       "  0.09275819036577429,\n",
       "  0.09205140109573093,\n",
       "  0.09180110687656065,\n",
       "  0.09109824693628721,\n",
       "  0.09105726006839959,\n",
       "  0.09058110607521873,\n",
       "  0.09026647646512305,\n",
       "  0.08989108545439585,\n",
       "  0.08971591873892716,\n",
       "  0.0892854674586228,\n",
       "  0.08879778278725489,\n",
       "  0.08837171055908716,\n",
       "  0.08807217324418681,\n",
       "  0.08784892926258699,\n",
       "  0.08756258679287775,\n",
       "  0.08715030385979583,\n",
       "  0.08705373172249112,\n",
       "  0.08660399828638346,\n",
       "  0.08618797292666777,\n",
       "  0.08596423294927394,\n",
       "  0.08568412996828555,\n",
       "  0.0854534194937774,\n",
       "  0.08511174363749366,\n",
       "  0.08470316236572606,\n",
       "  0.08459615909627503,\n",
       "  0.08450254521199635,\n",
       "  0.08404859501336304,\n",
       "  0.08369120806455611,\n",
       "  0.083406962454319,\n",
       "  0.08305243354822911,\n",
       "  0.08292228409222195,\n",
       "  0.08269922142582282],\n",
       " 'val_acc': [88.80580357142856,\n",
       "  90.91517857142856,\n",
       "  92.02008928571428,\n",
       "  92.57812500000001,\n",
       "  92.91294642857143,\n",
       "  93.31473214285712,\n",
       "  93.61607142857146,\n",
       "  93.81696428571429,\n",
       "  93.99553571428575,\n",
       "  94.19642857142856,\n",
       "  94.27455357142857,\n",
       "  94.39732142857142,\n",
       "  94.63169642857147,\n",
       "  94.76562500000001,\n",
       "  94.89955357142856,\n",
       "  94.95535714285717,\n",
       "  95.0111607142857,\n",
       "  95.11160714285712,\n",
       "  95.30133928571429,\n",
       "  95.234375,\n",
       "  95.37946428571426,\n",
       "  95.3794642857143,\n",
       "  95.31249999999999,\n",
       "  95.59151785714282,\n",
       "  95.54687499999999,\n",
       "  95.61383928571426,\n",
       "  95.63616071428574,\n",
       "  95.71428571428568,\n",
       "  95.81473214285715,\n",
       "  95.8035714285714,\n",
       "  95.90401785714288,\n",
       "  95.94866071428571,\n",
       "  96.01562500000001,\n",
       "  96.02678571428572,\n",
       "  95.98214285714288,\n",
       "  96.20535714285717,\n",
       "  96.03794642857143,\n",
       "  96.16071428571428,\n",
       "  96.21651785714289,\n",
       "  96.13839285714289,\n",
       "  96.20535714285712,\n",
       "  96.26116071428571,\n",
       "  96.30580357142857,\n",
       "  96.31696428571432,\n",
       "  96.31696428571425,\n",
       "  96.36160714285715,\n",
       "  96.38392857142854,\n",
       "  96.33928571428572,\n",
       "  96.43973214285718,\n",
       "  96.45089285714283,\n",
       "  96.41741071428571,\n",
       "  96.43973214285714,\n",
       "  96.45089285714286,\n",
       "  96.45089285714282,\n",
       "  96.50669642857142,\n",
       "  96.55133928571429,\n",
       "  96.5736607142857,\n",
       "  96.57366071428572,\n",
       "  96.54017857142858,\n",
       "  96.57366071428572,\n",
       "  96.59598214285715,\n",
       "  96.60714285714283,\n",
       "  96.66294642857142,\n",
       "  96.67410714285714,\n",
       "  96.75223214285714,\n",
       "  96.67410714285714,\n",
       "  96.77455357142857,\n",
       "  96.79687499999999,\n",
       "  96.86383928571428,\n",
       "  96.84151785714283,\n",
       "  96.86383928571433,\n",
       "  96.89732142857144,\n",
       "  96.94196428571429,\n",
       "  96.94196428571428,\n",
       "  96.95312500000001,\n",
       "  96.96428571428571,\n",
       "  96.93080357142857,\n",
       "  96.93080357142857,\n",
       "  97.05357142857137,\n",
       "  97.08705357142857,\n",
       "  96.98660714285714,\n",
       "  97.08705357142856,\n",
       "  97.09821428571429,\n",
       "  97.07589285714289,\n",
       "  97.04241071428572,\n",
       "  97.1205357142857,\n",
       "  97.14285714285714,\n",
       "  97.15401785714283,\n",
       "  97.14285714285717,\n",
       "  97.12053571428571,\n",
       "  97.17633928571428,\n",
       "  97.2098214285714,\n",
       "  97.22098214285717,\n",
       "  97.24330357142861,\n",
       "  97.22098214285715,\n",
       "  97.29910714285712,\n",
       "  97.28794642857144,\n",
       "  97.31026785714285,\n",
       "  97.28794642857143,\n",
       "  97.25446428571426],\n",
       " 'test_loss': -1,\n",
       " 'test_acc': -1}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b749152",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3caee613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3128017302070344\n",
      "90.90401785714286\n"
     ]
    }
   ],
   "source": [
    "print(train_state[\"test_loss\"])\n",
    "print(train_state[\"test_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "89d35c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):\n",
    "    \"\"\"Predict the rating of a review\n",
    "    \n",
    "    Args:\n",
    "        review (str): the text of the review\n",
    "        classifier (ReviewClassifier): the trained model\n",
    "        vectorizer (ReviewVectorizer): the corresponding vectorizer\n",
    "        decision_threshold (float): The numerical boundary which separates the rating classes\n",
    "    \"\"\"\n",
    "    review = cleaning_dataset(review)\n",
    "    \n",
    "    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n",
    "    result = classifier(vectorized_review.view(1, -1))\n",
    "    \n",
    "    probability_value = F.sigmoid(result).item()\n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index = 0\n",
    "\n",
    "    return vectorizer.rating_vocab.lookup_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "90b1b103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is out of the world -> 2\n"
     ]
    }
   ],
   "source": [
    "test_review = \"this is out of the world\"\n",
    "\n",
    "classifier = classifier.cpu()\n",
    "prediction = predict_rating(test_review, classifier, vectorizer, decision_threshold=0.5)\n",
    "print(\"{} -> {}\".format(test_review, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f06ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093481a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
