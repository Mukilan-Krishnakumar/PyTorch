{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "049fddc3",
   "metadata": {},
   "source": [
    "# Optimizing Model Parameters\n",
    "\n",
    "We are going to combine, everything we have done till now. We have loaded the Data and have defined the Neural Network. We also saw how to do `partial differentiation` with parameters. \n",
    "\n",
    "We are going to train the model. \n",
    "\n",
    "> Training model is an iterative process, which involves model making a guess about the output, calcualtes the error in the guess (loss), collects derivatives of error with respect to its parameters and optimizes the parameters with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8b3f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import Compose, Resize, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd2f0e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = Compose([\n",
    "    Resize([28,28]),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34fdbffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61f3c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.OxfordIIITPet(\n",
    "    root=\"data\",\n",
    "    split=\"trainval\",\n",
    "    download=True,\n",
    "    transform= transformations\n",
    "    )\n",
    "\n",
    "testing_data = datasets.OxfordIIITPet(\n",
    "    root = \"data\",\n",
    "    split = \"test\",\n",
    "    download = True,\n",
    "    transform = transformations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edb2f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e4de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(28*28*3, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 37),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e22e32a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PetNeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f14a7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PetNeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_stack): Sequential(\n",
       "    (0): Linear(in_features=2352, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=37, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a5f92",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "> Hyperparameters are adjustable parameters, can be tuned to increase model performance, training and convergance rates.\n",
    "\n",
    "Hyperparameters for Training:\n",
    "1. Number of Epochs - Iteration times\n",
    "2. Batch Size - Selection of input data before updation of parameters is based on `batch_size`\n",
    "3. Learning Rate - How much to update model parameters at each epoch/batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cda6ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea45a84",
   "metadata": {},
   "source": [
    "## Optimization Loop\n",
    "Each iteration of training is called the `epoch`.\n",
    "\n",
    "During each epoch:\n",
    "1. The Train Loop - Iterate over training dataset and try to converge to optimal parameters\n",
    "2. The Validation/Test Loop - Iterate over test dataset to check if model performance is improving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d00168",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "It measures the degree of dissimilarity between the obtained result and the target value. We want to minimize this. \n",
    "\n",
    "Common Loss Functions:\n",
    "1. `nn.MSELoss` - Mean Squared Error Loss for Regression Tasks\n",
    "2. `nn.NLLLoss` - Negative Log Likelihood Loss for Classification Tasks\n",
    "3. `nn.CrossEntropyLoss` - Combination of `nn.Softmax` and `nn.NLLLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac726b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453fe418",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "Optimization is the process of adjusting model parameters to reduce model error during training steps.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56ac53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d609d",
   "metadata": {},
   "source": [
    "Optimization happens in three steps for each iteration:\n",
    "1. Calls `optimizer.zero_grad()` - resets all gradients of model parameters to zero, to avoid double-counting.\n",
    "2. Backpropagate the prediction loss with `loss.backward()`. PyTorch deposits the gradients of the loss with respect to each parameter\n",
    "3. After getting gradients, we call `optimizer.step()` to adjust parameters by the gradients collected in backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5698a4ad",
   "metadata": {},
   "source": [
    "## Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff984369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Setting the model in train mode - Important for Batch Normalization and Dropout Layers\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        # Calculate Prediction and Loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch %100 == 0:\n",
    "            loss, current = loss.item(), (batch+1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Setting the model in test mode - Important for Batch Normalization and Dropout Layers\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e77074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.571516  [   64/ 3680]\n",
      "Test Error: \n",
      " Accuracy: 2.8%, Avg loss: 3.610824 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.571309  [   64/ 3680]\n",
      "Test Error: \n",
      " Accuracy: 2.8%, Avg loss: 3.610376 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.570958  [   64/ 3680]\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f6822b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
